---
title: "Lecture .mono[008]"
subtitle: "Ensembles ðŸŒ².smallest[ðŸŒ²]ðŸŒ².smallest[ðŸŽ„]ðŸŒ²"
author: "Edward Rubin"
#date: "`r format(Sys.time(), '%d %B %Y')`"
date: "Winter 2022"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---
exclude: true

```{r, setup, include = F}
library(pacman)
p_load(
  ISLR,
  broom, tidyverse,
  ggplot2, ggthemes, ggforce, ggridges, cowplot, scales, rayshader,
  latex2exp, viridis, extrafont, gridExtra, plotly, ggformula,
  DiagrammeR,
  kableExtra, DT, huxtable,
  data.table, dplyr, snakecase, janitor,
  lubridate, knitr,
  caret, tidymodels, baguette, rpart, rpart.plot, rattle,
  here, magrittr, parallel
)
# Define colors
red_pink   = "#e64173"
turquoise  = "#20B2AA"
orange     = "#FFA500"
red        = "#fb6107"
blue       = "#3b3b9a"
green      = "#8bb174"
grey_light = "grey70"
grey_mid   = "grey50"
grey_dark  = "grey20"
purple     = "#6A5ACD"
slate      = "#314f4f"
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(knitr.table.format = "html")
# A few extras
xaringanExtra::use_xaringan_extra(c("tile_view", "fit_screen"))
```

---
name: admin
# Admin
## Today

.note[Topic] Ensembles (applied to decision trees)

## Upcoming

.b[Readings]
- .note[Today] .it[ISL] Ch. 8.2
- .note[Next] .it[ISL] Ch. 9

.b[Project] Project topic and group due next week (Feb. 25th).

---
class: inverse, middle
# Decision trees
## Review

---
name: tree-review-fundamentals
# Decision trees
## Fundamentals

.attn[Decision trees]
- split the .it[predictor space] (our $\mathbf{X}$) into regions
- then predict the most-common value within a region

--


.col-left[
.hi-purple[Regression trees]
- .hi-slate[Predict:] Region's mean
- .hi-slate[Split:] Minimize RSS
- .hi-slate[Prune:] Penalized RSS
]

--

.col-right[
.hi-pink[Classification trees]
- .hi-slate[Predict:] Region's mode
- .hi-slate[Split:] Min. Gini or entropy.super
- .hi-slate[Prune:] Penalized error rate.super[ðŸŒ´]
]

.footnote[
ðŸŒ´ ... or Gini index or entropy
]

--

.clear-up[
An additional nuance for .attn[classification trees:] we typically care about the .b[proportions of classes in the leaves]â€”not just the final prediction.
]

---
class: clear

```{r, data-tree-example, include = F, cache = T}
# Data
rec_dt = rbindlist(list(
  data.table(1, 000, 010, 000, 100),
  data.table(2, 010, 035, 000, 043),
  data.table(3, 010, 035, 043, 100),
  data.table(4, 035, 090, 000, 085),
  data.table(5, 035, 090, 085, 100),
  data.table(6, 090, 100, 000, 027),
  data.table(7, 090, 100, 027, 055),
  data.table(8, 090, 100, 055, 100)
))
setnames(rec_dt, c("r", "xmin", "xmax", "ymin", "ymax"))
set.seed(13)
rec_dt[, val := runif(8)]
# Add labels
rec_dt[, r_label := paste0("R[", r, "]")]
# Remove ex_dt from memory
rm(ex_dt)
```

.ex[Example] Each split in our tree creates .hi-purple[regions].

```{r, plot-tree-example, echo = F, cache = T, dependson = "data-tree-example"}
# Plot
ggplot(
  data = rec_dt,
  aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax)
) +
geom_rect(fill = NA, color = purple) +
xlab(expression(x[1])) +
ylab(expression(x[2])) +
geom_text(
  aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = r_label),
  size = 6.5, family = "Fira Sans Book", color = purple, parse = T
) +
theme_minimal(base_size = 18, base_family = "Fira Sans Book")
```

---
class: clear

.ex[Example] Each region has its own .b[predicted value].

```{r, plot-tree-example2, echo = F, cache = T, dependson = "data-tree-example"}
# Plot
ggplot(
  data = rec_dt,
  aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax)
) +
geom_rect(aes(fill = val), color = "grey85", size = 0.5) +
xlab(expression(x[1])) +
ylab(expression(x[2])) +
geom_text(
  aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2, label = r_label, color = val > 0.75),
  size = 6.5, family = "Fira Sans Book", parse = T
) +
scale_fill_viridis_c(option = "magma") +
scale_color_manual(values = c("white", "black")) +
theme_minimal(base_size = 18, base_family = "Fira Sans Book") +
theme(legend.position = "none")
```

---
class: clear
exclude: true

```{r, plot-tree-example3, echo = F, cache = T, message = F, dependson = "data-tree-example", eval = FALSE}
# Plot
gg_regions = ggplot(
  data = rec_dt,
  aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax)
) +
geom_rect(aes(fill = val, color = val), size = 0.5) +
xlab(expression(x[1])) +
ylab(expression(x[2])) +
scale_fill_viridis_c(option = "magma") +
scale_color_viridis_c(option = "magma") +
theme_minimal(base_size = 18, base_family = "Fira Sans Book") +
theme(legend.position = "none")
# Pass to rayshader
plot_gg(
  gg_regions,
  zoom = 0.55,
  theta = -15,
  phi = 45,
  width = 6,
  windowsize = c(1400, 866),
  # sunangle = 225,
  multicore = T
)
render_snapshot(clear = TRUE)
```

---
name: tree-review-tradeoff
# Decision trees
## Strengths and weaknesses

As with any method, decision trees have tradeoffs.

--

.col-left.purple.small[
.b[Strengths]
<br>.b[+] Easily explained/interpretted
<br>.b[+] Include several graphical options
<br>.b[+] Mirror human decision making?
<br>.b[+] Handle num. or cat. on LHS/RHS.super[ðŸŒ³]
]

.footnote[
ðŸŒ³ Without needing to create lots of dummy variables!
<br>
.tran[ðŸŒ´ Blank]
]

--

.col-right.pink.small[
.b[Weaknesses]
<br>.b[-] Outperformed by other methods
<br>.b[-] Struggle with linearity
<br>.b[-] Can be very "non-robust"
]

.clear-up[
.attn[Non-robust:] Small data changes can cause huge changes in our tree.
]

--

.footnote[
.tran[ðŸŒ´ Blank]
<br>
ðŸŒ² Forests!
]

.note[Next:] Create ensembles of trees.super[ðŸŒ²] to strengthen these weaknesses.
--
.super[ðŸŒ´]

.footnote[
.tran[ðŸŒ´ Blank]
<br>
.tran[ðŸŒ² Forests!] ðŸŒ´ Which will also weaken some of the strengths.
]

---
layout: true
# Ensemble methods

---
class: inverse, middle

---
name: intro
## Intro

Rather than focusing on training a .b[single], highly accurate model,
<br>.attn[ensemble methods] combine .b[many] low-accuracy models into a .it[meta-model].

--

.note[Today:] Three common methods for .b[combining individual trees]

1. .attn[Bagging]
1. .attn[Random forests]
1. .attn[Boosting]

--

.b[Why?] While individual trees may be highly variable and inaccurate,
<br>a combination of trees is often quite stable and accurate.
--
.super[ðŸŒ²]

.footnote[
ðŸŒ² We will lose interpretability.
]

---
name: bag-intro
## Bagging

.attn[Bagging] creates additional samples via [.hi[bootstrapping]](https://raw.githack.com/edrubin/EC524W20/master/lecture/003/003-slides.html#62).

--

.qa[Q] How does bootstrapping help?

--

.qa[A] .note[Recall:] Individual decision trees suffer from variability (.it[non-robust]).

--

This .it[non-robustness] means trees can change .it[a lot] based upon which observations are included/excluded.

--

We're essentially using many "draws" instead of a single one..super[ðŸŒ´]

.footnote[
ðŸŒ´ Recall that an estimator's variance typically decreases as the sample size increases.
]

---
name: bag-algorithm
## Bagging

.attn[Bootstrap aggregation] (bagging) reduces this type of variability.

1. Create $B$ bootstrapped samples

1. Train an estimator (tree) $\color{#6A5ACD}{\mathop{\hat{f^b}}(x)}$ on each of the $B$ samples

1. Aggregate across your $B$ bootstrapped models:
$$
\begin{align}
  \color{#e64173}{\mathop{\hat{f}_{\text{bag}}}(x)} = \dfrac{1}{B}\sum_{b=1}^{B}\color{#6A5ACD}{\mathop{\hat{f^b}}(x)}
\end{align}
$$

This aggregated model $\color{#e64173}{\mathop{\hat{f}_{\text{bag}}}(x)}$ is your final model.

---
## Bagging trees

When we apply bagging to decision trees,

- we typically .hi-pink[grow the trees deep and do not prune]

- for .hi-purple[regression], we .hi-purple[average] across the $B$ trees' regions

- for .hi-purple[classification], we have more optionsâ€”but often take .hi-purple[plurality]

--

.hi-pink[Individual] (unpruned) trees will be very .hi-pink[flexible] and .hi-pink[noisy],
<br>but their .hi-purple[aggregate] will be quite .hi-purple[stable].

--

The number of trees $B$ is generally not critical with bagging.
<br> $B=100$ often works fine.

---
name: bag-oob
## Out-of-bag error estimation

Bagging also offers a convenient method for evaluating performance.

--

For any bootstrapped sample, we omit âˆ¼n/3 observations.

.attn[Out-of-bag (OOB) error estimation] estimates the test error rate using observations .b[randomly omitted] from each bootstrapped sample.

--

For each observation $i$:

1. Find all samples $S_i$ in which $i$ was omitted from training.

1. Aggregate the $|S_i|$ predictions $\color{#6A5ACD}{\mathop{\hat{f^b}}(x_i)}$, _e.g._, using their mean or mode

1. Calculate the error, _e.g._, $y_i - \mathop{\hat{f}_{i,\text{OOB},i}}(x_i)$

---
## Out-of-bag error estimation

When $B$ is big enough, the OOB error rate will be very close to LOOCV.

--

.qa[Q] Why use OOB error rate?

--

.qa[A] When $B$ and $n$ are large, cross validationâ€”with any number of foldsâ€”can become pretty computationally intensive.

---
class: clear, middle
layout: false

.note[Quick aside:] Here is a tool to search `parsnip` models:

>[https://www.tidymodels.org/find/parsnip/](https://www.tidymodels.org/find/parsnip/)

---
layout: true
# Ensemble methods

---
name: bag-r
## Bagging in R

We can use `tidymodels` plus the `baguette` package to bag trees.

--

.col-left[
.b[Function:] `bag_treebag()`
- "Specifies" model for `parsnip`.
]

.col-right[
```{r, eval = F}
# Train a bagged tree model
bag_tree(
  mode = "classification",
  cost_complexity = 0,
  tree_depth = NULL,
  min_n = 2,
  class_cost = NULL
)
```
]

---
count: false
## Bagging in R

We can use `tidymodels` plus the `baguette` package to bag trees.

.col-left[
.b[Function:] `bag_treebag()`
- "Specifies" model for `parsnip`.
- `mode`: class., reg., or unknown
]

.col-right[
```{r, eval = F}
# Train a bagged tree model
bag_tree(
  mode = "classification", #<<
  cost_complexity = 0,
  tree_depth = NULL,
  min_n = 2,
  class_cost = NULL
)
```
]

---
count: false
## Bagging in R

We can use `tidymodels` plus the `baguette` package to bag trees.

.col-left[
.b[Function:] `bag_treebag()`
- "Specifies" model for `parsnip`.
- `mode`: class., reg., or unknown
- `cost_complexity`: the penalty for model complexity (`Cp`)
]

.col-right[
```{r, eval = F}
# Train a bagged tree model
bag_tree(
  mode = "classification",
  cost_complexity = 0, #<<
  tree_depth = NULL,
  min_n = 2,
  class_cost = NULL
)
```
]

---
count: false
## Bagging in R

We can use `tidymodels` plus the `baguette` package to bag trees.

.col-left[
.b[Function:] `bag_treebag()`
- "Specifies" model for `parsnip`.
- `mode`: class., reg., or unknown
- `cost_complexity`: the penalty for model complexity (`Cp`)
- `tree_depth`: max. tree depth
]

.col-right[
```{r, eval = F}
# Train a bagged tree model
bag_tree(
  mode = "classification",
  cost_complexity = 0,
  tree_depth = NULL, #<<
  min_n = 2,
  class_cost = NULL
)
```
]

---
count: false
## Bagging in R

We can use `tidymodels` plus the `baguette` package to bag trees.

.col-left[
.b[Function:] `bag_treebag()`
- "Specifies" model for `parsnip`.
- `mode`: class., reg., or unknown
- `cost_complexity`: the penalty for model complexity (`Cp`)
- `tree_depth`: max. tree depth
- `min_n`: min. # obs. to split
]

.col-right[
```{r, eval = F}
# Train a bagged tree model
bag_tree(
  mode = "classification",
  cost_complexity = 0,
  tree_depth = NULL,
  min_n = 2, #<<
  class_cost = NULL
)
```
]

---
count: false
## Bagging in R

We can use `tidymodels` plus the `baguette` package to bag trees.

.col-left[
.b[Function:] `bag_treebag()`
- "Specifies" model for `parsnip`.
- `mode`: class., reg., or unknown
- `cost_complexity`: the penalty for model complexity (`Cp`)
- `tree_depth`: max. tree depth
- `min_n`: min. # obs. to split
- `class_cost`: magnify .note[cost] of incorrect guess (for first class)
]

.col-right[
```{r, eval = F}
# Train a bagged tree model
bag_tree(
  mode = "classification",
  cost_complexity = 0,
  tree_depth = NULL,
  min_n = 2,
  class_cost = NULL #<<
)
```
]

---
count: false
## Bagging in R

We can use `tidymodels` plus the `baguette` package to bag trees.

.col-left[
.b[Function:] `bag_treebag()`
- "Specifies" model for `parsnip`.
- `mode`: class., reg., or unknown
- `cost_complexity`: the penalty for model complexity (`Cp`)
- `tree_depth`: max. tree depth
- `min_n`: min. # obs. to split
- `class_cost`: magnify .note[cost]
- `rpart` is the defaul engine
]

.col-right[
```{r, eval = F}
# Train a bagged tree model
bag_tree(
  mode = "classification",
  cost_complexity = 0,
  tree_depth = NULL,
  min_n = 2,
  class_cost = NULL
) %>% set_engine(
  engine = "rpart", #<<
  times = 100
)
```
]

---
count: false
## Bagging in R

We can use `tidymodels` plus the `baguette` package to bag trees.

.col-left[
.b[Function:] `bag_treebag()`
- "Specifies" model for `parsnip`.
- `mode`: class., reg., or unknown
- `cost_complexity`: the penalty for model complexity (`Cp`)
- `tree_depth`: max. tree depth
- `min_n`: min. # obs. to split
- `class_cost`: magnify .note[cost]
- `rpart` is the defaul engine
- `times`: the number of trees
]

.col-right[
```{r, eval = F}
# Train a bagged tree model
bag_tree(
  mode = "classification",
  cost_complexity = 0,
  tree_depth = NULL,
  min_n = 2,
  class_cost = NULL
) %>% set_engine(
  engine = "rpart",
  times = 100 #<<
)
```
]

---
exclude: true

## Example: Bagging in R

```{r, load-data-heart, include = F, cache = T}
# Read data
heart_df = read_csv("Heart.csv") %>%
  dplyr::select(-1) %>%
  rename(HeartDisease = AHD) %>%
  clean_names()
# Impute missing values
heart_recipe = recipe(heart_disease ~ ., data = heart_df) %>% 
  step_impute_median(all_predictors() & all_numeric()) %>% 
  step_impute_mode(all_predictors() & all_nominal())
heart_df = heart_recipe %>% prep() %>% juice()
```

.col-left[
<br>With OOB-based error
```{r, ex-bag-oob, cache = T, dependson = "load-data-heart"}
# Set the seed
set.seed(12345)
# Train the bagged trees
heart_bag = train(
  heart_disease ~ .,
  data = heart_df,
  method = "treebag",
  nbagg = 100,
  keepX = T,
  trControl = trainControl(
    method = "oob" #<<
  )
)
```
]
.col-right[
<br>With CV-based error
```{r, ex-bag-cv, eval = F}
# Set the seed
set.seed(12345)
# Train the bagged trees
heart_bag_cv = train(
  heart_disease ~ .,
  data = heart_df,
  method = "treebag",
  nbagg = 100,
  keepX = T,
  trControl = trainControl(
    method = "cv", #<<
    number = 5 #<<
  )
)
```
]

---
exclude: true

```{r, sim-bag-size, cache = T}
# Set the seed
set.seed(12345)
# Train the bagged trees
bag_oob = mclapply(
  X = 2:300,
  mc.cores = 12,
  FUN = function(n) {
    train(
      heart_disease ~ .,
      data = heart_df,
      method = "treebag",
      nbagg = n,
      keepX = T,
      trControl = trainControl(
        method = "oob"
      )
    )$results$Accuracy %>%
    data.frame(accuracy = ., n_trees = n)
  }
) %>% bind_rows()
# Train the bagged trees
bag_cv = mclapply(
  X = 2:300,
  mc.cores = 12,
  FUN = function(n) {
    train(
      heart_disease ~ .,
      data = heart_df,
      method = "treebag",
      nbagg = n,
      keepX = T,
      trControl = trainControl(
        method = "cv",
        number = 5
      )
    )$results$Accuracy %>%
    data.frame(accuracy = ., n_trees = n)
  }
) %>% bind_rows()
```

---
layout: false
class: clear

.b[Bagging and the number of trees]

```{r, plot-bag, echo = F, cache = T}
ggplot(
  data = bind_rows(
    bag_oob %>% mutate(type = "Bagged, OOB"),
    bag_cv %>% mutate(type = "Bagged, CV")
  ),
  aes(x = n_trees, y = accuracy, color = type)
) +
geom_line() +
scale_y_continuous("Accuracy", labels = scales::percent) +
scale_x_continuous("Number of trees") +
scale_color_manual("[Method, Estimate]", values = c(red_pink, purple)) +
theme_minimal(base_size = 20, base_family = "Fira Sans Book") +
theme(legend.position = "bottom") +
coord_cartesian(ylim = c(0.60, 0.90))
```

---
class: clear, middle

Unfortunately, this combination of `rpart`/`baguette`/`parsnip`/`yardstick` doesn't ([currently](https://github.com/tidymodels/baguette/issues/33)) offer OOB-based metrics. ðŸ˜ž

---
class: clear, middle

We .it[can] ["trick"](https://www.sds.pub/bagged-trees.html) random forests (`ranger`) into doing OOB for bagged trees.

But first, we need to learn about random forests...

---
class: clear, middle

... and before .it[that], let's briefly talk about variable importance.

---
name: bag-var
# Ensemble methods
## Variable importance

While ensemble methods tend to .hi[improve predictive performance],
<br>they also tend .hi[reduce interpretability].

--

We can illustrate .attn[variables' importance] by considering their splits' reductions in the model's performance metric (RSS, Gini, entropy, _etc._)..super[ðŸŒ³]

.footnote[
ðŸŒ³ This idea isn't exclusive to bagging/ensembles; it also works for a single tree.
]

--

.note[Note] By default, many variable-importance functions will scale importance.

---
layout: false
class: clear

In the case of `"rpart"` bagged trees...

```{r, tidy-bag-0, eval = T, include = F, cache = T}
# Read data
heart_df = read_csv("Heart.csv") %>%
    dplyr::select(-1) %>%
  rename(HeartDisease = AHD) %>%
  clean_names()
```

```{r, tidy-bag-1, eval = T, cache = T, dependson = -1}
# Recipe to clean data (impute NAs)
heart_recipe = recipe(heart_disease ~ ., data = heart_df) %>% 
  step_medianimpute(all_predictors() & all_numeric()) %>% 
  step_modeimpute(all_predictors() & all_nominal())
# Define the bagged tree model
heart_bag = bag_tree(
  mode = "classification",
  cost_complexity = 0,
  tree_depth = NULL,
  min_n = 2,
  class_cost = NULL
) %>% set_engine(
  engine = "rpart",
  times = 100
)
# Define workflow
heart_bag_wf = workflow() %>% 
  add_model(heart_bag) %>% 
  add_recipe(heart_recipe)
# Fit/assess with CV
heart_bag_fit = heart_bag_wf %>% fit(heart_df)
```

---
class: clear

... the fitted object automatically includes variable importance.

```{r, tidy-bag-imp, echo = F}
# Variable importance
heart_bag_fit %>% extract_fit_parsnip() %$% fit
```

---
class: clear

```{r, ex-var-importance, include = F, cache = T, dependson = "ex-bag-oob"}
# Get importance
imp_df = heart_bag_fit %>% extract_fit_parsnip() %$% fit %>% var_imp()
# Standardize importance
imp_df %<>% mutate(
  importance = value - min(value),
  importance = 100 * importance / max(importance)
)
```

.hi-pink[Variable importance] from our bagged tree model.

```{r, plot-var-importance, echo = F, dependson = -3}
# Plot importance
ggplot(
  data = imp_df,
  aes(x = reorder(term, -importance), y = importance)
) +
geom_col(fill = red_pink) +
geom_hline(yintercept = 0) +
xlab("Variable") +
ylab("Importance (scaled [0,100])") +
# scale_fill_viridis_c(option = "magma", direction = -1) +
theme_minimal(base_size = 20, base_family = "Fira Sans Book") +
theme(legend.position = "none") +
coord_flip()
```

---
name: bag-weak
# Ensemble methods
## Bagging

Bagging has one additional shortcoming...

If one variable dominates other variables, the .hi[trees will be very correlated].

--

If the trees are very correlated, then bagging loses its advantage.

--

.note[Solution] We should make the trees less correlated.

---
layout: true
# Ensemble methods

---
name: rf-intro
## Random forests

.attn[Random forests] improve upon bagged trees by .it[decorrelating] the trees.

--

In order to decorrelate its trees, a .attn[random forest] only .pink[considers a random subset of] $\color{#e64173}{m\enspace (\approx\sqrt{p})}$ .pink[predictors] when making each split (for each tree).

--

Restricting the variables our tree sees at a given split

--

- nudges trees away from always using the same variables,

--

- increasing the variation across trees in our forest,

--

- which potentially reduces the variance of our estimates.

--

If our predictors are very correlated, we may want to shrink $m$.

---
## Random forests

Random forests thus introduce .b[two dimensions of random variation]

1. the .b[bootstrapped sample]

2. the $m$ .b[randomly selected predictors] (for the split)

Everything else about random forests works just as it did with bagging..super[ðŸŽ„]

.footnote[
ðŸŽ„ And just as it did with plain, old decision trees.
]


---
name: rf-r
## Random forests in R

You have several [options](http://topepo.github.io/caret/train-models-by-tag.html#Random_Forest) for training random forests with `tidymodels`..super[ðŸŽ„]
<br>_E.g._, `ranger`, `randomForest`, `spark`.

`rand_forest()` accesses each of these packages via their .it[engines].

.footnote[
ðŸŽ„ And even more if you look outside of `tidymodels`.
]

--

- The default engine is `"ranger"` ([`ranger`](https://cran.r-project.org/web/packages/ranger/index.html) package).

--

- The argument `mtry` gives $m$, the # of predictors at each split.

--

You've already seen the other hyperparameters for `ranger`:

- `trees` the number of trees in your (random) forest
- `min_n` min. # of observations

---
layout: true
# Ensemble methods

Training a random forest in R using `tidymodels`...

---

.col-left[
... and `ranger`
]

.col-right[
```{r, ex-tidy-ranger, cache = T}
# Define the random forest
heart_rf = rand_forest(
  mode = "classification",
  mtry = 3,
  trees = 100,
  min_n = 2
) %>% set_engine(
  engine = "ranger",
  splitrule = "gini"
)
```
]

```{r, ex-tidy-ranger-fit, include = F, cache = T, dependson = -1}
# Define workflow
heart_rf_wf = workflow() %>% 
  add_model(heart_rf) %>% 
  add_recipe(heart_recipe)
# Fit
heart_rf_fit = heart_rf_wf %>% fit(heart_df)
```

---
count: false

.col-left[
... and `ranger`
- Goal: Classification
]

.col-right[
```{r, eval = F}
# Define the random forest
heart_rf = rand_forest(
  mode = "classification", #<<
  mtry = 3,
  trees = 100,
  min_n = 2
) %>% set_engine(
  engine = "ranger",
  splitrule = "gini"
)
```
]

---
count: false

.col-left[
... and `ranger`
- Goal: Classification
- Three variables per split
]

.col-right[
```{r, eval = F}
# Define the random forest
heart_rf = rand_forest(
  mode = "classification", 
  mtry = 3, #<<
  trees = 100,
  min_n = 2
) %>% set_engine(
  engine = "ranger",
  splitrule = "gini"
)
```
]

---
count: false

.col-left[
... and `ranger`
- Goal: Classification
- Three variables per split
- 100 trees in the forest
]

.col-right[
```{r, eval = F}
# Define the random forest
heart_rf = rand_forest(
  mode = "classification", 
  mtry = 3,
  trees = 100, #<<
  min_n = 2
) %>% set_engine(
  engine = "ranger",
  splitrule = "gini"
)
```
]

---
count: false

.col-left[
... and `ranger`
- Goal: Classification
- Three variables per split
- 100 trees in the forest
- At least 2 obs. to split
]

.col-right[
```{r, eval = F}
# Define the random forest
heart_rf = rand_forest(
  mode = "classification", 
  mtry = 3,
  trees = 100,
  min_n = 2 #<<
) %>% set_engine(
  engine = "ranger",
  splitrule = "gini"
)
```
]

---
count: false

.col-left[
... and `ranger`
- Goal: Classification
- Three variables per split
- 100 trees in the forest
- At least 2 obs. to split
- Choose the `ranger` engine
]

.col-right[
```{r, eval = F}
# Define the random forest
heart_rf = rand_forest(
  mode = "classification", 
  mtry = 3,
  trees = 100,
  min_n = 2
) %>% set_engine(
  engine = "ranger", #<<
  splitrule = "gini"
)
```
]

---
count: false

.col-left[
... and `ranger`
- Goal: Classification
- Three variables per split
- 100 trees in the forest
- At least 2 obs. to split
- Choose the `ranger` engine
- Set a [splitting rule](https://dials.tidymodels.org/reference/ranger_parameters.html)
]

.col-right[
```{r, eval = F}
# Define the random forest
heart_rf = rand_forest(
  mode = "classification", 
  mtry = 3,
  trees = 100,
  min_n = 2
) %>% set_engine(
  engine = "ranger",
  splitrule = "gini" #<<
)
```
]

---
layout: false
class: clear

.note[Step 1:] Define our parameter grid

```{r, rf-param, cache = T}
# Define the parameter grid
rf_grid = expand_grid(
  mtry = 1:13,
  min_n = 1:15
)
```

---
class: clear

.note[Step 2:] Write a function that fits a RF using .b.orange[given hyperparameters].

```{r, rf-fun, cache = T}
# Function: One set of hyperparam
rf_i = function(i) {
  # Define the random forest
  heart_rf_i = rand_forest(
    mode = "classification", 
    mtry = rf_grid$mtry[i], #<<
    trees = 100,
    min_n = rf_grid$min_n[i] #<<
  ) %>% set_engine(engine = "ranger", splitrule = "gini")
  # Define workflow
  heart_rf_wf_i = 
    workflow() %>% add_model(heart_rf_i) %>% add_recipe(heart_recipe)
  # Fit
  heart_rf_fit_i = heart_rf_wf_i %>% fit(heart_df)
  # Return DF w/ OOB error and the hyperparameters
  tibble(
    mtry = rf_grid$mtry[i],
    min_n = rf_grid$min_n[i],
    # Note: OOB error is buried
    error_oob = heart_rf_fit_i$fit$fit$fit$prediction.error
  )
}
```

---
layout: false
class: clear

.note[Step 3:] Fit all of the forests (in `parallel`)!

```{r, rf-param-oob, cache = T, dependson = c(-2, -1)}
# Fit the RFs on the grid
rf_tuning = mclapply(
  X = 1:nrow(rf_grid),
  FUN = rf_i,
  mc.cores = 12
) %>% rbindlist()
```

---
layout: false
class: clear

.b[Accuracy] (OOB) across the grid of our parameters.

```{r, plot-rf-parameters, echo = F}
ggplot(
  data = rf_tuning,
  aes(x = mtry, y = min_n, fill = 1-error_oob)
) +
geom_tile(color = "white", size = 0.3) +
scale_x_continuous("Number of variables at split (m)", breaks = 1:13) +
scale_y_continuous("Min. leaf size", breaks = c(1, 5, 10, 15)) +
scale_fill_viridis_c("Accuracy", option = "magma", labels = percent) +
theme_minimal(base_size = 20, base_family = "Fira Sans Book") +
theme(
  legend.position = "bottom",
  legend.key.width = unit(3, "cm"),
  panel.grid.minor = element_blank()
)
```

---
class: clear
exclude: true

.col-left[
```{r, sim-forest-size, cache = T}
# Read data
heart_df = read_csv("Heart.csv") %>%
  dplyr::select(-1) %>%
  rename(HeartDisease = AHD) %>%
  clean_names()
# Impute missing values
heart_recipe = recipe(heart_disease ~ ., data = heart_df) %>% 
  step_impute_median(all_predictors() & all_numeric()) %>% 
  step_impute_mode(all_predictors() & all_nominal())
heart_df = heart_recipe %>% prep() %>% juice()
# Set the seed
set.seed(12345)
# Train the bagged trees
rf_oob = mclapply(
  X = 2:300,
  mc.cores = 12,
  FUN = function(n) {
    train(
      heart_disease ~ .,
      data = heart_df,
      method = "ranger",
      num.trees = n,
      trControl = trainControl(
        method = "oob"
      ),
      tuneGrid = data.frame(
        "mtry" = 2,
        "splitrule" = "gini",
        "min.node.size" = 4
      )
    )$finalModel$prediction.error %>% subtract(1, .) %>%
    data.frame(accuracy = ., n_trees = n)
  }
) %>% bind_rows()
```
]

.col-right[
```{r, sim-forest-size2, cache = T}
# Set seed
set.seed(6789)
# Train the bagged trees
rf_cv = mclapply(
  X = 2:300,
  mc.cores = 12,
  FUN = function(n) {
    train(
      heart_disease ~ .,
      data = heart_df,
      method = "ranger",
      num.trees = n,
      trControl = trainControl(
        method = "cv",
        number = 5
      ),
      tuneGrid = data.frame(
        "mtry" = 2,
        "splitrule" = "gini",
        "min.node.size" = 4
      )
    )$finalModel$prediction.error %>% subtract(1, .) %>%
    data.frame(accuracy = ., n_trees = n)
  }
) %>% bind_rows()
```
]

---
class: clear

.b[Tree ensembles and the number of trees]

```{r, plot-bag-rf, echo = F}
ggplot(
  data = bind_rows(
    bag_oob %>% mutate(type = "Bagged, OOB"),
    bag_cv %>% mutate(type = "Bagged, CV"),
    rf_oob %>% mutate(type = "Random forest, OOB"),
    rf_cv %>% mutate(type = "Random forest, CV")
  ),
  aes(x = n_trees, y = accuracy, color = type)
) +
geom_line() +
scale_y_continuous("Accuracy", labels = scales::percent) +
scale_x_continuous("Number of trees") +
scale_color_manual(
  "[Method, Estimate]",
  values = c(red_pink, purple, orange, slate)
) +
theme_minimal(base_size = 20, base_family = "Fira Sans Book") +
theme(legend.position = "bottom") +
coord_cartesian(ylim = c(0.60, 0.90))
```

---
layout: true
# Ensemble methods

---
name: boost-intro
## Boosting

So far, the elements of our ensembles have been acting independently:
<br> any single tree knows nothing about the rest of the forest.

--

.attn[Boosting] allows trees to pass on information to eachother.

--

Specifically, .attn[boosting] trains its trees.super[ðŸŒ²] .it[sequentially]â€”each new tree trains on the residuals (mistakes) from its predecessors.

.footnote[
ðŸŒ² As with bagging, boosting can be applied to many methods (in addition to trees).
]

--

- We add each new tree to our model $\hat{f}$ (and update our residuals).

- Trees are typically smallâ€”slowly improving $\hat{f}$ .it[where it struggles].

---
name: boost-param
## Boosting

Boosting has three .hi[tuning parameters].

1. The .hi[number of trees] $\color{#e64173}{B}$ can be important to prevent overfitting.

--

1. The .hi[shrinkage parameter] $\color{#e64173}{\lambda}$, which controls boosting's .it[learning rate] (often 0.01 or 0.001).

--

1. The .hi[number of splits] $\color{#e64173}{d}$ in each tree (trees' complexity).
--

  - Individual trees are typically shortâ€”often $d=1$ ("stumps").

  - .note[Remember] Trees learn from predecessors' mistakes,<br>so no single tree needs to offer a perfect model.
---
name: boost-alg
## How to boost

.hi-purple[Step 1:] Set $\color{#6A5ACD}{\mathop{\hat{f}}}(x) = 0$, which yields residuals $r_i = y_i$ for all $i$.

--

.hi-pink[Step 2:] For $\color{#e64173}{b} = 1,\,2\,\ldots,\, B$ do:

.move-right[
.b[A.] Fit a tree $\color{#e64173}{\hat{f^b}}$ with $d$ splits.
]

--

.move-right[
.b[B.] Update the model $\color{#6A5ACD}{\hat{f}}$ with "shrunken version" of new treee $\color{#e64173}{\hat{f^b}}$
]

$$
\begin{align}
  \color{#6A5ACD}{\mathop{\hat{f}}}(x) \leftarrow \color{#6A5ACD}{\mathop{\hat{f}}}(x) + \lambda \mathop{\color{#e64173}{\hat{f^b}}}(x)
\end{align}
$$

--

.move-right[
.b[C.] Update the residuals: $r_i \leftarrow r_i - \lambda \mathop{\color{#e64173}{\hat{f^b}}}(x)$.
]

--

.hi-orange[Step 3:] Output the boosted model:
$\mathop{\color{#6A5ACD}{\hat{f}}}(x) = \sum_{b} \lambda \mathop{\color{#e64173}{\hat{f^b}}}(x)$.
---
name: boost-r
## Boosting in R

We will use `parsnips`'s `boost_tree()` to train boosted trees..super[ðŸŒ´]

.footnote[
ðŸŒ´ This method uses the `xgboost` package.
]

`boost_tree()` takes several parameters you've seenâ€”plus one more:

1. `mtry` number of predictors to try at each split

1. `trees`, the number of trees $(B)$

1. `min_n`, minimum observations to split

1. `tree_depth`, max. tree depth (max. splits from top)

1. `learn_rate`, the learning rate $(\lambda)$

---
exclude: true

```{r, ex-boost, cache = T, message = F}
# Set the seed
set.seed(12345)
# Train the random forest
heart_boost = train(
  heart_disease ~ .,
  data = heart_df,
  method = "gbm",
  trControl = trainControl(
    method = "cv",
    number = 5
  ),
  tuneGrid = expand.grid(
    "n.trees" = seq(1, 300, by = 1),
    "interaction.depth" = 1:3,
    "shrinkage" = c(0.1, 0.01, 0.001),
    "n.minobsinnode" = 5
  )
)
```

```{r, ex-boost-new, cache = T, message = F}
# Set the seed
set.seed(12345)
# Train the random forest
heart_boost = train(
  heart_disease ~ .,
  data = heart_df,
  method = "gbm",
  trControl = trainControl(
    method = "cv",
    number = 5
  ),
  tuneGrid = expand.grid(
    "n.trees" = seq(1, 1e3, by = 1),
    "interaction.depth" = 1:3,
    "shrinkage" = c(0.1, 0.01, 0.001),
    "n.minobsinnode" = 5
  )
)
```

---
exclude: true

## Boosting in R

.col-left.pad-top[
```{r, eval = F}
# Set the seed
set.seed(12345)
# Train the random forest
heart_boost = train(
  heart_disease ~ .,
  data = heart_df,
  method = "gbm",
  trControl = trainControl(
    method = "cv",
    number = 5
  ),
  tuneGrid = expand.grid(
    "n.trees" = seq(25, 200, by = 25),
    "interaction.depth" = 1:3,
    "shrinkage" = c(0.1, 0.01, 0.001),
    "n.minobsinnode" = 5
  )
)
```
]

---
exclude: true
count: false

## Boosting in R

.col-left.pad-top[
```{r, eval = F}
# Set the seed
set.seed(12345)
# Train the random forest
heart_boost = train(
  heart_disease ~ .,
  data = heart_df,
  method = "gbm", #<<
  trControl = trainControl(
    method = "cv",
    number = 5
  ),
  tuneGrid = expand.grid(
    "n.trees" = seq(25, 200, by = 25),
    "interaction.depth" = 1:3,
    "shrinkage" = c(0.1, 0.01, 0.001),
    "n.minobsinnode" = 5
  )
)
```
]
.col-right.pad-top[
<br>
- boosted trees via `gbm` package
]

---
exclude: true
count: false

## Boosting in R

.col-left.pad-top[
```{r, eval = F}
# Set the seed
set.seed(12345)
# Train the random forest
heart_boost = train(
  heart_disease ~ .,
  data = heart_df,
  method = "gbm",
  trControl = trainControl(
    method = "cv", #<<
    number = 5 #<<
  ),
  tuneGrid = expand.grid(
    "n.trees" = seq(25, 200, by = 25),
    "interaction.depth" = 1:3,
    "shrinkage" = c(0.1, 0.01, 0.001),
    "n.minobsinnode" = 5
  )
)
```
]
.col-right.pad-top[
<br>
- boosted trees via `gbm` package
- cross validation now (no OOB)
]

---
exclude: true
count: false

## Boosting in R

.col-left.pad-top[
```{r, eval = F}
# Set the seed
set.seed(12345)
# Train the random forest
heart_boost = train(
  heart_disease ~ .,
  data = heart_df,
  method = "gbm",
  trControl = trainControl(
    method = "cv",
    number = 5
  ),
  tuneGrid = expand.grid(
    "n.trees" = seq(25, 200, by = 25), #<<
    "interaction.depth" = 1:3,
    "shrinkage" = c(0.1, 0.01, 0.001),
    "n.minobsinnode" = 5
  )
)
```
]
.col-right.pad-top[
<br>
- boosted trees via `gbm` package
- cross validation now (no OOB)
- CV-search of parameter grid
  - number of trees
]

---
exclude: true
count: false

## Boosting in R

.col-left.pad-top[
```{r, eval = F}
# Set the seed
set.seed(12345)
# Train the random forest
heart_boost = train(
  heart_disease ~ .,
  data = heart_df,
  method = "gbm",
  trControl = trainControl(
    method = "cv",
    number = 5
  ),
  tuneGrid = expand.grid(
    "n.trees" = seq(25, 200, by = 25),
    "interaction.depth" = 1:3, #<<
    "shrinkage" = c(0.1, 0.01, 0.001),
    "n.minobsinnode" = 5
  )
)
```
]
.col-right.pad-top[
<br>
- boosted trees via `gbm` package
- cross validation now (no OOB)
- CV-search of parameter grid
  - number of trees
  - tree depth (complexity)
]

---
exclude: true
count: false

## Boosting in R

.col-left.pad-top[
```{r, eval = F}
# Set the seed
set.seed(12345)
# Train the random forest
heart_boost = train(
  heart_disease ~ .,
  data = heart_df,
  method = "gbm",
  trControl = trainControl(
    method = "cv",
    number = 5
  ),
  tuneGrid = expand.grid(
    "n.trees" = seq(25, 200, by = 25),
    "interaction.depth" = 1:3,
    "shrinkage" = c(0.1, 0.01, 0.001), #<<
    "n.minobsinnode" = 5
  )
)
```
]
.col-right.pad-top[
<br>
- boosted trees via `gbm` package
- cross validation now (no OOB)
- CV-search of parameter grid
  - number of trees
  - tree depth (complexity)
  - shrinkage (learing rate)
]

---
exclude: true
count: false

## Boosting in R

.col-left.pad-top[
```{r, eval = F}
# Set the seed
set.seed(12345)
# Train the random forest
heart_boost = train(
  heart_disease ~ .,
  data = heart_df,
  method = "gbm",
  trControl = trainControl(
    method = "cv",
    number = 5
  ),
  tuneGrid = expand.grid(
    "n.trees" = seq(25, 200, by = 25),
    "interaction.depth" = 1:3,
    "shrinkage" = c(0.1, 0.01, 0.001),
    "n.minobsinnode" = 5 #<<
  )
)
```
]
.col-right.pad-top[
<br>
- boosted trees via `gbm` package
- cross validation now (no OOB)
- CV-search of parameter grid
  - number of trees
  - tree depth (complexity)
  - shrinkage (learing rate)
  - minimum leaf size<br>(not searching here)
]

---
layout: false
class: clear

.b[Comparing boosting parameters]â€”notice the rates of learning

```{r, plot-boost-param, echo = F}
ggplot(
  data = heart_boost$results %>% filter(n.trees <= 300) %>% mutate(grp = paste(shrinkage, interaction.depth, sep = ", ")),
  aes(
    x = n.trees,
    y = Accuracy,
    color = as.character(interaction.depth),
    linetype = as.character(shrinkage)
  )
) +
geom_vline(xintercept = 204, size = 1.3, alpha = 0.3, color = red_pink) +
geom_line(size = 0.4) +
scale_y_continuous("Accuracy", labels = percent) +
scale_x_continuous("Number of trees") +
scale_color_viridis_d("Tree depth", option = "magma", end = 0.85) +
scale_linetype_manual("Shrinkage", values = c("longdash", "dotted", "solid")) +
theme_minimal(base_size = 18, base_family = "Fira Sans Book")
```

---
layout: false
class: clear

.b[Comparing boosting parameters]â€”more trees!

```{r, plot-boost-param-more, echo = F}
ggplot(
  data = heart_boost$results %>% mutate(grp = paste(shrinkage, interaction.depth, sep = ", ")),
  aes(
    x = n.trees,
    y = Accuracy,
    color = as.character(interaction.depth),
    linetype = as.character(shrinkage)
  )
) +
geom_vline(xintercept = 204, size = 1.3, alpha = 0.15, color = red_pink) +
geom_vline(xintercept = 606, size = 1.3, alpha = 0.3, color = red_pink) +
geom_line(size = 0.4) +
scale_y_continuous("Accuracy", labels = percent) +
scale_x_continuous("Number of trees") +
scale_color_viridis_d("Tree depth", option = "magma", end = 0.85) +
scale_linetype_manual("Shrinkage", values = c("longdash", "dotted", "solid")) +
theme_minimal(base_size = 18, base_family = "Fira Sans Book")
```

---
class: clear

.b[Tree ensembles and the number of trees]

```{r, plot-bag-rf-boost, echo = F}
ggplot(
  data = bind_rows(
    bag_oob %>% mutate(type = "Bagged, OOB"),
    bag_cv %>% mutate(type = "Bagged, CV"),
    rf_oob %>% mutate(type = "RF, OOB"),
    rf_cv %>% mutate(type = "RF, CV"),
    heart_boost$results %>% filter(
      shrinkage == 0.1 &
      interaction.depth == 1 &
      between(n.trees, 2, 300)
    ) %>% transmute(accuracy = Accuracy, n_trees = n.trees, type = "Boosted, CV")
  ),
  aes(x = n_trees, y = accuracy, color = type, size = type)
) +
geom_line() +
scale_y_continuous("Accuracy", labels = scales::percent) +
scale_x_continuous("Number of trees") +
scale_color_manual(
  "[Method, Estimate]",
  values = c(red_pink, purple, turquoise, orange, slate)
) +
scale_size_manual(
  "[Method, Estimate]",
  values = c(0.25, 0.25, 0.7, 0.25, 0.25)
) +
theme_minimal(base_size = 18, base_family = "Fira Sans Book") +
theme(legend.position = "bottom") +
coord_cartesian(ylim = c(0.60, 0.90))
```

---
layout: false
class: clear, middle

Of course, there are a lot of other tree-based learning options:

- [CatBoost](https://catboost.ai) ([R](https://catboost.ai/en/docs/concepts/r-installation))

- [LightGBM](https://lightgbm.readthedocs.io/en/latest/) ([R](https://lightgbm.readthedocs.io/en/latest/R/index.html))

- [TabNet](https://arxiv.org/abs/1908.07442) ([R](https://github.com/mlverse/tabnet/))


---
name: sources
layout: false
# Sources

These notes draw upon

- [An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/) (*ISL*)<br>James, Witten, Hastie, and Tibshirani
---
# Table of contents

.col-left[
.smallest[
#### Admin
- [Today and upcoming](#admin)

#### Decision trees

1. [Fundamentals](#tree-review-fundamentals)
1. [Strengths and weaknesses](#tree-review-tradeoff)

#### Other
- [Sources/references](#sources)

]
]
.col-right[
.smallest[

#### Ensemble methods

1. [Introduction](#intro)
1. [Bagging](#bag-intro)
  - [Introduction](#bag-intro)
  - [Algorithm](#bag-algorithm)
  - [Out-of-bag](#bag-oob)
  - [In R](#bag-r)
  - [Variable importance](#bag-var)
1. [Random forests](#rf-intro)
  - [Introduction](#rf-intro)
  - [In R](#rf-r)
1. [Boosting](#boost-intro)
  - [Introduction](#boost-intro)
  - [Parameters](#boost-param)
  - [Algorithm](#boost-alg)
  - [In R](#boost-r)

]
]

```{r, create pdfs, eval = F, include = F}
pagedown::chrome_print("008-slides.html", timeout = 300, wait = 4)
```